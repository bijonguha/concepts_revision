{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/bijonguha/concepts_revision/blob/main/decision_tree_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier from Scratch\n",
    "\n",
    "In this notebook, we will explore a custom implementation of a Decision Tree classifier in Python. We will dissect the provided code, understand each component, and demonstrate how to use the classifier on a real dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Introduction to Decision Trees](#introduction)\n",
    "2. [Implementation Overview](#overview)\n",
    "3. [Code Breakdown](#breakdown)\n",
    "    - [TreeNode Class](#treenode)\n",
    "    - [DecisionTree Class](#decisiontree)\n",
    "4. [Training and Prediction](#training)\n",
    "5. [Feature Importance](#feature-importance)\n",
    "6. [Example with Iris Dataset](#example)\n",
    "7. [Conclusion](#conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"introduction\"></a>\n",
    "## 1. Introduction to Decision Trees\n",
    "\n",
    "Decision Trees are a popular machine learning algorithm used for classification and regression tasks. They work by recursively splitting the dataset based on feature values to create a tree-like model of decisions.\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "- **Root Node:** The top node representing the entire dataset.\n",
    "- **Internal Nodes:** Nodes that represent a feature and a threshold to split the data.\n",
    "- **Leaf Nodes:** Nodes that represent the final output or prediction.\n",
    "- **Information Gain:** A metric to choose the best feature and threshold for splitting by measuring the reduction in entropy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"overview\"></a>\n",
    "## 2. Implementation Overview\n",
    "\n",
    "The provided implementation consists of two main classes:\n",
    "\n",
    "1. **TreeNode:** Represents each node in the Decision Tree, holding information about the split and predictions.\n",
    "2. **DecisionTree:** The main classifier that builds the tree, trains on data, and makes predictions.\n",
    "\n",
    "Below is the complete code for both classes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class TreeNode():\n",
    "    def __init__(self, data, feature_idx, feature_val, prediction_probs, information_gain) -> None:\n",
    "        self.data = data\n",
    "        self.feature_idx = feature_idx\n",
    "        self.feature_val = feature_val\n",
    "        self.prediction_probs = prediction_probs\n",
    "        self.information_gain = information_gain\n",
    "        self.feature_importance = self.data.shape[0] * self.information_gain\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "    def node_def(self) -> str:\n",
    "\n",
    "        if (self.left or self.right):\n",
    "            return f\"NODE | Information Gain = {self.information_gain} | Split IF X[{self.feature_idx}] < {self.feature_val} THEN left O/W right\"\n",
    "        else:\n",
    "            unique_values, value_counts = np.unique(self.data[:,-1], return_counts=True)\n",
    "            output = \", \".join([f\"{value}->{count}\" for value, count in zip(unique_values, value_counts)])            \n",
    "            return f\"LEAF | Label Counts = {output} | Pred Probs = {self.prediction_probs}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    \"\"\"\n",
    "    Decision Tree Classifier\n",
    "    Training: Use \"train\" function with train set features and labels\n",
    "    Predicting: Use \"predict\" function with test set features\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_depth=4, min_samples_leaf=1, \n",
    "                 min_information_gain=0.0, numb_of_features_splitting=None,\n",
    "                 amount_of_say=None) -> None:\n",
    "        \"\"\"\n",
    "        Setting the class with hyperparameters\n",
    "        max_depth: (int) -> max depth of the tree\n",
    "        min_samples_leaf: (int) -> min # of samples required to be in a leaf to make the splitting possible\n",
    "        min_information_gain: (float) -> min information gain required to make the splitting possible\n",
    "        num_of_features_splitting: (str) ->  when splitting if sqrt then sqrt(# of features) features considered, \n",
    "                                                            if log then log(# of features) features considered\n",
    "                                                            else all features are considered\n",
    "        amount_of_say: (float) -> used for Adaboost algorithm                                                    \n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.min_information_gain = min_information_gain\n",
    "        self.numb_of_features_splitting = numb_of_features_splitting\n",
    "        self.amount_of_say = amount_of_say\n",
    "\n",
    "    def _entropy(self, class_probabilities: list) -> float:\n",
    "        return sum([-p * np.log2(p) for p in class_probabilities if p>0])\n",
    "    \n",
    "    def _class_probabilities(self, labels: list) -> list:\n",
    "        total_count = len(labels)\n",
    "        return [label_count / total_count for label_count in Counter(labels).values()]\n",
    "\n",
    "    def _data_entropy(self, labels: list) -> float:\n",
    "        return self._entropy(self._class_probabilities(labels))\n",
    "    \n",
    "    def _partition_entropy(self, subsets: list) -> float:\n",
    "        \"\"\"subsets = list of label lists (EX: [[1,0,0], [1,1,1])\"\"\"\n",
    "        total_count = sum([len(subset) for subset in subsets])\n",
    "        return sum([self._data_entropy(subset) * (len(subset) / total_count) for subset in subsets])\n",
    "    \n",
    "    def _split(self, data: np.array, feature_idx: int, feature_val: float) -> tuple:\n",
    "        \n",
    "        mask_below_threshold = data[:, feature_idx] < feature_val\n",
    "        group1 = data[mask_below_threshold]\n",
    "        group2 = data[~mask_below_threshold]\n",
    "\n",
    "        return group1, group2\n",
    "    \n",
    "    def _select_features_to_use(self, data: np.array) -> list:\n",
    "        \"\"\"\n",
    "        Randomly selects the features to use while splitting w.r.t. hyperparameter numb_of_features_splitting\n",
    "        \"\"\"\n",
    "        feature_idx = list(range(data.shape[1]-1))\n",
    "\n",
    "        if self.numb_of_features_splitting == \"sqrt\":\n",
    "            feature_idx_to_use = np.random.choice(feature_idx, size=int(np.sqrt(len(feature_idx))))\n",
    "        elif self.numb_of_features_splitting == \"log\":\n",
    "            feature_idx_to_use = np.random.choice(feature_idx, size=int(np.log2(len(feature_idx))))\n",
    "        else:\n",
    "            feature_idx_to_use = feature_idx\n",
    "\n",
    "        return feature_idx_to_use\n",
    "        \n",
    "    def _find_best_split(self, data: np.array) -> tuple:\n",
    "        \"\"\"\n",
    "        Finds the best split (with the lowest entropy) given data\n",
    "        Returns 2 splitted groups and split information\n",
    "        \"\"\"\n",
    "        min_part_entropy = 1e9\n",
    "        feature_idx_to_use =  self._select_features_to_use(data)\n",
    "\n",
    "        for idx in feature_idx_to_use:\n",
    "            feature_vals = np.percentile(data[:, idx], q=np.arange(25, 100, 25))\n",
    "            for feature_val in feature_vals:\n",
    "                g1, g2, = self._split(data, idx, feature_val)\n",
    "                part_entropy = self._partition_entropy([g1[:, -1], g2[:, -1]])\n",
    "                if part_entropy < min_part_entropy:\n",
    "                    min_part_entropy = part_entropy\n",
    "                    min_entropy_feature_idx = idx\n",
    "                    min_entropy_feature_val = feature_val\n",
    "                    g1_min, g2_min = g1, g2\n",
    "\n",
    "        return g1_min, g2_min, min_entropy_feature_idx, min_entropy_feature_val, min_part_entropy\n",
    "\n",
    "    def _find_label_probs(self, data: np.array) -> np.array:\n",
    "\n",
    "        labels_as_integers = data[:,-1].astype(int)\n",
    "        # Calculate the total number of labels\n",
    "        total_labels = len(labels_as_integers)\n",
    "        # Calculate the ratios (probabilities) for each label\n",
    "        label_probabilities = np.zeros(len(self.labels_in_train), dtype=float)\n",
    "\n",
    "        # Populate the label_probabilities array based on the specific labels\n",
    "        for i, label in enumerate(self.labels_in_train):\n",
    "            label_index = np.where(labels_as_integers == i)[0]\n",
    "            if len(label_index) > 0:\n",
    "                label_probabilities[i] = len(label_index) / total_labels\n",
    "\n",
    "        return label_probabilities\n",
    "\n",
    "    def _create_tree(self, data: np.array, current_depth: int) -> TreeNode:\n",
    "        \"\"\"\n",
    "        Recursive, depth first tree creation algorithm\n",
    "        \"\"\"\n",
    "\n",
    "        # Check if the max depth has been reached (stopping criteria)\n",
    "        if current_depth > self.max_depth:\n",
    "            return None\n",
    "        \n",
    "        # Find best split\n",
    "        split_1_data, split_2_data, split_feature_idx, split_feature_val, split_entropy = self._find_best_split(data)\n",
    "        \n",
    "        # Find label probs for the node\n",
    "        label_probabilities = self._find_label_probs(data)\n",
    "\n",
    "        # Calculate information gain\n",
    "        node_entropy = self._entropy(label_probabilities)\n",
    "        information_gain = node_entropy - split_entropy\n",
    "        \n",
    "        # Create node\n",
    "        node = TreeNode(data, split_feature_idx, split_feature_val, label_probabilities, information_gain)\n",
    "\n",
    "        # Check if the min_samples_leaf has been satisfied (stopping criteria)\n",
    "        if self.min_samples_leaf > split_1_data.shape[0] or self.min_samples_leaf > split_2_data.shape[0]:\n",
    "            return node\n",
    "        # Check if the min_information_gain has been satisfied (stopping criteria)\n",
    "        elif information_gain < self.min_information_gain:\n",
    "            return node\n",
    "\n",
    "        current_depth += 1\n",
    "        node.left = self._create_tree(split_1_data, current_depth)\n",
    "        node.right = self._create_tree(split_2_data, current_depth)\n",
    "        \n",
    "        return node\n",
    "    \n",
    "    def _predict_one_sample(self, X: np.array) -> np.array:\n",
    "        \"\"\"Returns prediction for 1 dim array\"\"\"\n",
    "        node = self.tree\n",
    "\n",
    "        # Finds the leaf which X belongs\n",
    "        while node:\n",
    "            pred_probs = node.prediction_probs\n",
    "            if X[node.feature_idx] < node.feature_val:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "\n",
    "        return pred_probs\n",
    "\n",
    "    def train(self, X_train: np.array, Y_train: np.array) -> None:\n",
    "        \"\"\"\n",
    "        Trains the model with given X and Y datasets\n",
    "        \"\"\"\n",
    "\n",
    "        # Concat features and labels\n",
    "        self.labels_in_train = np.unique(Y_train)\n",
    "        train_data = np.concatenate((X_train, np.reshape(Y_train, (-1, 1))), axis=1)\n",
    "\n",
    "        # Start creating the tree\n",
    "        self.tree = self._create_tree(data=train_data, current_depth=0)\n",
    "\n",
    "        # Calculate feature importance\n",
    "        self.feature_importances = dict.fromkeys(range(X_train.shape[1]), 0)\n",
    "        self._calculate_feature_importance(self.tree)\n",
    "        # Normalize the feature importance values\n",
    "        self.feature_importances = {k: v / total for total in (sum(self.feature_importances.values()),) for k, v in self.feature_importances.items()}\n",
    "\n",
    "    def predict_proba(self, X_set: np.array) -> np.array:\n",
    "        \"\"\"Returns the predicted probs for a given data set\"\"\"\n",
    "\n",
    "        pred_probs = np.apply_along_axis(self._predict_one_sample, 1, X_set)\n",
    "        \n",
    "        return pred_probs\n",
    "\n",
    "    def predict(self, X_set: np.array) -> np.array:\n",
    "        \"\"\"Returns the predicted labels for a given data set\"\"\"\n",
    "\n",
    "        pred_probs = self.predict_proba(X_set)\n",
    "        preds = np.argmax(pred_probs, axis=1)\n",
    "        \n",
    "        return preds    \n",
    "        \n",
    "    def _print_recursive(self, node: TreeNode, level=0) -> None:\n",
    "        if node != None:\n",
    "            self._print_recursive(node.left, level + 1)\n",
    "            print('    ' * 4 * level + '-> ' + node.node_def())\n",
    "            self._print_recursive(node.right, level + 1)\n",
    "\n",
    "    def print_tree(self) -> None:\n",
    "        self._print_recursive(node=self.tree)\n",
    "\n",
    "    def _calculate_feature_importance(self, node):\n",
    "        \"\"\"Calculates the feature importance by visiting each node in the tree recursively\"\"\"\n",
    "        if node != None:\n",
    "            self.feature_importances[node.feature_idx] += node.feature_importance\n",
    "            self._calculate_feature_importance(node.left)\n",
    "            self._calculate_feature_importance(node.right)         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"breakdown\"></a>\n",
    "## 3. Code Breakdown\n",
    "\n",
    "Let's delve into each component of the implementation to understand how the Decision Tree works.\n",
    "\n",
    "<a id=\"treenode\"></a>\n",
    "### 3.1. TreeNode Class\n",
    "\n",
    "The `TreeNode` class represents each node in the Decision Tree. It holds information about the data at that node, the feature used for splitting, the threshold value, prediction probabilities, information gain, and pointers to its left and right child nodes.\n",
    "\n",
    "```python\n",
    "class TreeNode():\n",
    "    def __init__(self, data, feature_idx, feature_val, prediction_probs, information_gain) -> None:\n",
    "        self.data = data\n",
    "        self.feature_idx = feature_idx\n",
    "        self.feature_val = feature_val\n",
    "        self.prediction_probs = prediction_probs\n",
    "        self.information_gain = information_gain\n",
    "        self.feature_importance = self.data.shape[0] * self.information_gain\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "    def node_def(self) -> str:\n",
    "\n",
    "        if (self.left or self.right):\n",
    "            return f\"NODE | Information Gain = {self.information_gain} | Split IF X[{self.feature_idx}] < {self.feature_val} THEN left O/W right\"\n",
    "        else:\n",
    "            unique_values, value_counts = np.unique(self.data[:,-1], return_counts=True)\n",
    "            output = \", \".join([f\"{value}->{count}\" for value, count in zip(unique_values, value_counts)])            \n",
    "            return f\"LEAF | Label Counts = {output} | Pred Probs = {self.prediction_probs}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attributes:**\n",
    "\n",
    "- `data`: The subset of the dataset at the current node.\n",
    "- `feature_idx`: Index of the feature used for splitting.\n",
    "- `feature_val`: Threshold value for splitting.\n",
    "- `prediction_probs`: Probability distribution of classes at the node.\n",
    "- `information_gain`: Information gain achieved by the split.\n",
    "- `feature_importance`: Importance of the feature based on information gain.\n",
    "- `left` and `right`: Pointers to the left and right child nodes.\n",
    "\n",
    "**Methods:**\n",
    "\n",
    "- `node_def()`: Returns a string representation of the node, differentiating between internal nodes and leaf nodes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"decisiontree\"></a>\n",
    "### 3.2. DecisionTree Class\n",
    "\n",
    "The `DecisionTree` class encapsulates the logic for building, training, and using the Decision Tree classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "class DecisionTree():\n",
    "    \"\"\"\n",
    "    Decision Tree Classifier\n",
    "    Training: Use \"train\" function with train set features and labels\n",
    "    Predicting: Use \"predict\" function with test set features\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_depth=4, min_samples_leaf=1, \n",
    "                 min_information_gain=0.0, numb_of_features_splitting=None,\n",
    "                 amount_of_say=None) -> None:\n",
    "        \"\"\"\n",
    "        Setting the class with hyperparameters\n",
    "        max_depth: (int) -> max depth of the tree\n",
    "        min_samples_leaf: (int) -> min # of samples required to be in a leaf to make the splitting possible\n",
    "        min_information_gain: (float) -> min information gain required to make the splitting possible\n",
    "        numb_of_features_splitting: (str) -> when splitting if 'sqrt' then sqrt(# of features) features considered, \n",
    "                                           if 'log' then log(# of features) features considered\n",
    "                                           else all features are considered\n",
    "        amount_of_say: (float) -> used for Adaboost algorithm                                                    \n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.min_information_gain = min_information_gain\n",
    "        self.numb_of_features_splitting = numb_of_features_splitting\n",
    "        self.amount_of_say = amount_of_say\n",
    "\n",
    "    def _entropy(self, class_probabilities: list) -> float:\n",
    "        return sum([-p * np.log2(p) for p in class_probabilities if p > 0])\n",
    "    \n",
    "    def _class_probabilities(self, labels: list) -> list:\n",
    "        total_count = len(labels)\n",
    "        return [label_count / total_count for label_count in Counter(labels).values()]\n",
    "\n",
    "    def _data_entropy(self, labels: list) -> float:\n",
    "        return self._entropy(self._class_probabilities(labels))\n",
    "    \n",
    "    def _partition_entropy(self, subsets: list) -> float:\n",
    "        \"\"\"subsets = list of label lists (EX: [[1,0,0], [1,1,1]])\"\"\"\n",
    "        total_count = sum([len(subset) for subset in subsets])\n",
    "        return sum([self._data_entropy(subset) * (len(subset) / total_count) for subset in subsets])\n",
    "    \n",
    "    def _split(self, data: np.array, feature_idx: int, feature_val: float) -> tuple:\n",
    "        mask_below_threshold = data[:, feature_idx] < feature_val\n",
    "        group1 = data[mask_below_threshold]\n",
    "        group2 = data[~mask_below_threshold]\n",
    "\n",
    "        return group1, group2\n",
    "    \n",
    "    def _select_features_to_use(self, data: np.array) -> list:\n",
    "        \"\"\"\n",
    "        Randomly selects the features to use while splitting with respect to the hyperparameter numb_of_features_splitting\n",
    "        \"\"\"\n",
    "        feature_idx = list(range(data.shape[1]-1))\n",
    "\n",
    "        if self.numb_of_features_splitting == \"sqrt\":\n",
    "            feature_idx_to_use = np.random.choice(feature_idx, size=int(np.sqrt(len(feature_idx))), replace=False)\n",
    "        elif self.numb_of_features_splitting == \"log\":\n",
    "            feature_idx_to_use = np.random.choice(feature_idx, size=int(np.log2(len(feature_idx))), replace=False)\n",
    "        else:\n",
    "            feature_idx_to_use = feature_idx\n",
    "\n",
    "        return feature_idx_to_use\n",
    "        \n",
    "    def _find_best_split(self, data: np.array) -> tuple:\n",
    "        \"\"\"\n",
    "        Finds the best split (with the lowest entropy) given data\n",
    "        Returns 2 splitted groups and split information\n",
    "        \"\"\"\n",
    "        min_part_entropy = 1e9\n",
    "        feature_idx_to_use = self._select_features_to_use(data)\n",
    "\n",
    "        for idx in feature_idx_to_use:\n",
    "            feature_vals = np.percentile(data[:, idx], q=np.arange(25, 100, 25))\n",
    "            for feature_val in feature_vals:\n",
    "                g1, g2 = self._split(data, idx, feature_val)\n",
    "                if len(g1) == 0 or len(g2) == 0:\n",
    "                    continue\n",
    "                part_entropy = self._partition_entropy([g1[:, -1], g2[:, -1]])\n",
    "                if part_entropy < min_part_entropy:\n",
    "                    min_part_entropy = part_entropy\n",
    "                    min_entropy_feature_idx = idx\n",
    "                    min_entropy_feature_val = feature_val\n",
    "                    g1_min, g2_min = g1, g2\n",
    "\n",
    "        return g1_min, g2_min, min_entropy_feature_idx, min_entropy_feature_val, min_part_entropy\n",
    "\n",
    "    def _find_label_probs(self, data: np.array) -> np.array:\n",
    "        labels_as_integers = data[:, -1].astype(int)\n",
    "        # Calculate the total number of labels\n",
    "        total_labels = len(labels_as_integers)\n",
    "        # Calculate the ratios (probabilities) for each label\n",
    "        label_probabilities = np.zeros(len(self.labels_in_train), dtype=float)\n",
    "\n",
    "        # Populate the label_probabilities array based on the specific labels\n",
    "        for i, label in enumerate(self.labels_in_train):\n",
    "            label_index = np.where(labels_as_integers == i)[0]\n",
    "            if len(label_index) > 0:\n",
    "                label_probabilities[i] = len(label_index) / total_labels\n",
    "\n",
    "        return label_probabilities\n",
    "\n",
    "    def _create_tree(self, data: np.array, current_depth: int) -> TreeNode:\n",
    "        \"\"\"\n",
    "        Recursive, depth-first tree creation algorithm\n",
    "        \"\"\"\n",
    "\n",
    "        # Check if the max depth has been reached (stopping criteria)\n",
    "        if current_depth > self.max_depth:\n",
    "            return None\n",
    "        \n",
    "        # Find best split\n",
    "        split_1_data, split_2_data, split_feature_idx, split_feature_val, split_entropy = self._find_best_split(data)\n",
    "        \n",
    "        # Find label probs for the node\n",
    "        label_probabilities = self._find_label_probs(data)\n",
    "\n",
    "        # Calculate information gain\n",
    "        node_entropy = self._entropy(label_probabilities)\n",
    "        information_gain = node_entropy - split_entropy\n",
    "        \n",
    "        # Create node\n",
    "        node = TreeNode(data, split_feature_idx, split_feature_val, label_probabilities, information_gain)\n",
    "\n",
    "        # Check if the min_samples_leaf has been satisfied (stopping criteria)\n",
    "        if self.min_samples_leaf > split_1_data.shape[0] or self.min_samples_leaf > split_2_data.shape[0]:\n",
    "            return node\n",
    "        # Check if the min_information_gain has been satisfied (stopping criteria)\n",
    "        elif information_gain < self.min_information_gain:\n",
    "            return node\n",
    "\n",
    "        current_depth += 1\n",
    "        node.left = self._create_tree(split_1_data, current_depth)\n",
    "        node.right = self._create_tree(split_2_data, current_depth)\n",
    "        \n",
    "        return node\n",
    "    \n",
    "    def _predict_one_sample(self, X: np.array) -> np.array:\n",
    "        \"\"\"Returns prediction for 1D array\"\"\"\n",
    "        node = self.tree\n",
    "\n",
    "        # Finds the leaf which X belongs to\n",
    "        while node:\n",
    "            pred_probs = node.prediction_probs\n",
    "            if X[node.feature_idx] < node.feature_val:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "\n",
    "        return pred_probs\n",
    "\n",
    "    def train(self, X_train: np.array, Y_train: np.array) -> None:\n",
    "        \"\"\"\n",
    "        Trains the model with given X and Y datasets\n",
    "        \"\"\"\n",
    "\n",
    "        # Concatenate features and labels\n",
    "        self.labels_in_train = np.unique(Y_train)\n",
    "        train_data = np.concatenate((X_train, np.reshape(Y_train, (-1, 1))), axis=1)\n",
    "\n",
    "        # Start creating the tree\n",
    "        self.tree = self._create_tree(data=train_data, current_depth=0)\n",
    "\n",
    "        # Calculate feature importance\n",
    "        self.feature_importances = dict.fromkeys(range(X_train.shape[1]), 0)\n",
    "        self._calculate_feature_importance(self.tree)\n",
    "        # Normalize the feature importance values\n",
    "        total = sum(self.feature_importances.values())\n",
    "        if total > 0:\n",
    "            self.feature_importances = {k: v / total for k, v in self.feature_importances.items()}\n",
    "        else:\n",
    "            self.feature_importances = {k: 0 for k in self.feature_importances}\n",
    "\n",
    "    def predict_proba(self, X_set: np.array) -> np.array:\n",
    "        \"\"\"Returns the predicted probabilities for a given data set\"\"\"\n",
    "\n",
    "        pred_probs = np.apply_along_axis(self._predict_one_sample, 1, X_set)\n",
    "        \n",
    "        return pred_probs\n",
    "\n",
    "    def predict(self, X_set: np.array) -> np.array:\n",
    "        \"\"\"Returns the predicted labels for a given data set\"\"\"\n",
    "\n",
    "        pred_probs = self.predict_proba(X_set)\n",
    "        preds = np.argmax(pred_probs, axis=1)\n",
    "        \n",
    "        return preds    \n",
    "    \n",
    "    def _print_recursive(self, node: TreeNode, level=0) -> None:\n",
    "        if node is not None:\n",
    "            self._print_recursive(node.left, level + 1)\n",
    "            print('    ' * 4 * level + '-> ' + node.node_def())\n",
    "            self._print_recursive(node.right, level + 1)\n",
    "\n",
    "    def print_tree(self) -> None:\n",
    "        self._print_recursive(node=self.tree)\n",
    "\n",
    "    def _calculate_feature_importance(self, node):\n",
    "        \"\"\"Calculates the feature importance by visiting each node in the tree recursively\"\"\"\n",
    "        if node is not None:\n",
    "            self.feature_importances[node.feature_idx] += node.feature_importance\n",
    "            self._calculate_feature_importance(node.left)\n",
    "            self._calculate_feature_importance(node.right)         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Parameters:**\n",
    "\n",
    "- `max_depth`: Maximum depth of the tree.\n",
    "- `min_samples_leaf`: Minimum number of samples required to be at a leaf node.\n",
    "- `min_information_gain`: Minimum information gain required to split.\n",
    "- `numb_of_features_splitting`: Strategy to select the number of features to consider when looking for the best split (`'sqrt'`, `'log'`, or `None` for all features).\n",
    "- `amount_of_say`: Parameter used for algorithms like AdaBoost (not utilized in this implementation).\n",
    "\n",
    "**Core Methods:**\n",
    "\n",
    "1. **Entropy Calculation:**\n",
    "\n",
    "    ```python\n",
    "    def _entropy(self, class_probabilities: list) -> float:\n",
    "        return sum([-p * np.log2(p) for p in class_probabilities if p > 0])\n",
    "    ```\n",
    "    Calculates the entropy given a list of class probabilities.\n",
    "\n",
    "2. **Class Probabilities:**\n",
    "\n",
    "    ```python\n",
    "    def _class_probabilities(self, labels: list) -> list:\n",
    "        total_count = len(labels)\n",
    "        return [label_count / total_count for label_count in Counter(labels).values()]\n",
    "    ```\n",
    "    Computes the probability distribution of classes in the provided labels.\n",
    "\n",
    "3. **Data Entropy:**\n",
    "\n",
    "    ```python\n",
    "    def _data_entropy(self, labels: list) -> float:\n",
    "        return self._entropy(self._class_probabilities(labels))\n",
    "    ```\n",
    "    Calculates the entropy of a dataset based on its labels.\n",
    "\n",
    "4. **Partition Entropy:**\n",
    "\n",
    "    ```python\n",
    "    def _partition_entropy(self, subsets: list) -> float:\n",
    "        \"\"\"subsets = list of label lists (EX: [[1,0,0], [1,1,1]])\"\"\"\n",
    "        total_count = sum([len(subset) for subset in subsets])\n",
    "        return sum([self._data_entropy(subset) * (len(subset) / total_count) for subset in subsets])\n",
    "    ```\n",
    "    Calculates the weighted entropy of partitions (subsets) created by a split.\n",
    "\n",
    "5. **Data Splitting:**\n",
    "\n",
    "    ```python\n",
    "    def _split(self, data: np.array, feature_idx: int, feature_val: float) -> tuple:\n",
    "        mask_below_threshold = data[:, feature_idx] < feature_val\n",
    "        group1 = data[mask_below_threshold]\n",
    "        group2 = data[~mask_below_threshold]\n",
    "\n",
    "        return group1, group2\n",
    "    ```\n",
    "    Splits the data into two groups based on a feature index and a threshold value.\n",
    "\n",
    "6. **Feature Selection:**\n",
    "\n",
    "    ```python\n",
    "    def _select_features_to_use(self, data: np.array) -> list:\n",
    "        \"\"\"\n",
    "        Randomly selects the features to use while splitting with respect to the hyperparameter numb_of_features_splitting\n",
    "        \"\"\"\n",
    "        feature_idx = list(range(data.shape[1]-1))\n",
    "\n",
    "        if self.numb_of_features_splitting == \"sqrt\":\n",
    "            feature_idx_to_use = np.random.choice(feature_idx, size=int(np.sqrt(len(feature_idx))), replace=False)\n",
    "        elif self.numb_of_features_splitting == \"log\":\n",
    "            feature_idx_to_use = np.random.choice(feature_idx, size=int(np.log2(len(feature_idx))), replace=False)\n",
    "        else:\n",
    "            feature_idx_to_use = feature_idx\n",
    "\n",
    "        return feature_idx_to_use\n",
    "    ```\n",
    "    Selects a subset of features to consider for splitting based on the `numb_of_features_splitting` parameter.\n",
    "\n",
    "7. **Finding the Best Split:**\n",
    "\n",
    "    ```python\n",
    "    def _find_best_split(self, data: np.array) -> tuple:\n",
    "        \"\"\"\n",
    "        Finds the best split (with the lowest entropy) given data\n",
    "        Returns 2 splitted groups and split information\n",
    "        \"\"\"\n",
    "        min_part_entropy = 1e9\n",
    "        feature_idx_to_use = self._select_features_to_use(data)\n",
    "\n",
    "        for idx in feature_idx_to_use:\n",
    "            feature_vals = np.percentile(data[:, idx], q=np.arange(25, 100, 25))\n",
    "            for feature_val in feature_vals:\n",
    "                g1, g2 = self._split(data, idx, feature_val)\n",
    "                if len(g1) == 0 or len(g2) == 0:\n",
    "                    continue\n",
    "                part_entropy = self._partition_entropy([g1[:, -1], g2[:, -1]])\n",
    "                if part_entropy < min_part_entropy:\n",
    "                    min_part_entropy = part_entropy\n",
    "                    min_entropy_feature_idx = idx\n",
    "                    min_entropy_feature_val = feature_val\n",
    "                    g1_min, g2_min = g1, g2\n",
    "\n",
    "        return g1_min, g2_min, min_entropy_feature_idx, min_entropy_feature_val, min_part_entropy\n",
    "    ```\n",
    "    Iterates over selected features and possible threshold values to find the split that results in the lowest partition entropy.\n",
    "\n",
    "8. **Label Probabilities:**\n",
    "\n",
    "    ```python\n",
    "    def _find_label_probs(self, data: np.array) -> np.array:\n",
    "        labels_as_integers = data[:, -1].astype(int)\n",
    "        # Calculate the total number of labels\n",
    "        total_labels = len(labels_as_integers)\n",
    "        # Calculate the ratios (probabilities) for each label\n",
    "        label_probabilities = np.zeros(len(self.labels_in_train), dtype=float)\n",
    "\n",
    "        # Populate the label_probabilities array based on the specific labels\n",
    "        for i, label in enumerate(self.labels_in_train):\n",
    "            label_index = np.where(labels_as_integers == i)[0]\n",
    "            if len(label_index) > 0:\n",
    "                label_probabilities[i] = len(label_index) / total_labels\n",
    "\n",
    "        return label_probabilities\n",
    "    ```\n",
    "    Calculates the probability distribution of labels at the current node.\n",
    "\n",
    "9. **Tree Construction:**\n",
    "\n",
    "    ```python\n",
    "    def _create_tree(self, data: np.array, current_depth: int) -> TreeNode:\n",
    "        \"\"\"\n",
    "        Recursive, depth-first tree creation algorithm\n",
    "        \"\"\"\n",
    "\n",
    "        # Check if the max depth has been reached (stopping criteria)\n",
    "        if current_depth > self.max_depth:\n",
    "            return None\n",
    "        \n",
    "        # Find best split\n",
    "        split_1_data, split_2_data, split_feature_idx, split_feature_val, split_entropy = self._find_best_split(data)\n",
    "        \n",
    "        # Find label probs for the node\n",
    "        label_probabilities = self._find_label_probs(data)\n",
    "\n",
    "        # Calculate information gain\n",
    "        node_entropy = self._entropy(label_probabilities)\n",
    "        information_gain = node_entropy - split_entropy\n",
    "        \n",
    "        # Create node\n",
    "        node = TreeNode(data, split_feature_idx, split_feature_val, label_probabilities, information_gain)\n",
    "\n",
    "        # Check if the min_samples_leaf has been satisfied (stopping criteria)\n",
    "        if self.min_samples_leaf > split_1_data.shape[0] or self.min_samples_leaf > split_2_data.shape[0]:\n",
    "            return node\n",
    "        # Check if the min_information_gain has been satisfied (stopping criteria)\n",
    "        elif information_gain < self.min_information_gain:\n",
    "            return node\n",
    "\n",
    "        current_depth += 1\n",
    "        node.left = self._create_tree(split_1_data, current_depth)\n",
    "        node.right = self._create_tree(split_2_data, current_depth)\n",
    "        \n",
    "        return node\n",
    "    ```\n",
    "    Recursively builds the tree by finding the best splits and creating child nodes until stopping criteria are met (e.g., maximum depth, minimum samples per leaf, minimum information gain).\n",
    "\n",
    "10. **Prediction for a Single Sample:**\n",
    "\n",
    "    ```python\n",
    "    def _predict_one_sample(self, X: np.array) -> np.array:\n",
    "        \"\"\"Returns prediction for 1D array\"\"\"\n",
    "        node = self.tree\n",
    "\n",
    "        # Finds the leaf which X belongs to\n",
    "        while node:\n",
    "            pred_probs = node.prediction_probs\n",
    "            if X[node.feature_idx] < node.feature_val:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "\n",
    "        return pred_probs\n",
    "    ```\n",
    "    Traverses the tree based on feature values of a single sample to predict class probabilities.\n",
    "\n",
    "11. **Training the Model:**\n",
    "\n",
    "    ```python\n",
    "    def train(self, X_train: np.array, Y_train: np.array) -> None:\n",
    "        \"\"\"\n",
    "        Trains the model with given X and Y datasets\n",
    "        \"\"\"\n",
    "\n",
    "        # Concatenate features and labels\n",
    "        self.labels_in_train = np.unique(Y_train)\n",
    "        train_data = np.concatenate((X_train, np.reshape(Y_train, (-1, 1))), axis=1)\n",
    "\n",
    "        # Start creating the tree\n",
    "        self.tree = self._create_tree(data=train_data, current_depth=0)\n",
    "\n",
    "        # Calculate feature importance\n",
    "        self.feature_importances = dict.fromkeys(range(X_train.shape[1]), 0)\n",
    "        self._calculate_feature_importance(self.tree)\n",
    "        # Normalize the feature importance values\n",
    "        total = sum(self.feature_importances.values())\n",
    "        if total > 0:\n",
    "            self.feature_importances = {k: v / total for k, v in self.feature_importances.items()}\n",
    "        else:\n",
    "            self.feature_importances = {k: 0 for k in self.feature_importances}\n",
    "    ```\n",
    "    Prepares the training data, builds the tree, and computes feature importance based on information gain.\n",
    "\n",
    "12. **Making Predictions:**\n",
    "\n",
    "    ```python\n",
    "    def predict_proba(self, X_set: np.array) -> np.array:\n",
    "        \"\"\"Returns the predicted probabilities for a given data set\"\"\"\n",
    "\n",
    "        pred_probs = np.apply_along_axis(self._predict_one_sample, 1, X_set)\n",
    "        \n",
    "        return pred_probs\n",
    "\n",
    "    def predict(self, X_set: np.array) -> np.array:\n",
    "        \"\"\"Returns the predicted labels for a given data set\"\"\"\n",
    "\n",
    "        pred_probs = self.predict_proba(X_set)\n",
    "        preds = np.argmax(pred_probs, axis=1)\n",
    "        \n",
    "        return preds    \n",
    "    ```\n",
    "    Provides methods to predict class probabilities and final class labels for a dataset.\n",
    "\n",
    "13. **Printing the Tree:**\n",
    "\n",
    "    ```python\n",
    "    def _print_recursive(self, node: TreeNode, level=0) -> None:\n",
    "        if node is not None:\n",
    "            self._print_recursive(node.left, level + 1)\n",
    "            print('    ' * 4 * level + '-> ' + node.node_def())\n",
    "            self._print_recursive(node.right, level + 1)\n",
    "\n",
    "    def print_tree(self) -> None:\n",
    "        self._print_recursive(node=self.tree)\n",
    "    ```\n",
    "    Recursively prints the tree structure, indicating splits and leaf nodes.\n",
    "\n",
    "14. **Calculating Feature Importance:**\n",
    "\n",
    "    ```python\n",
    "    def _calculate_feature_importance(self, node):\n",
    "        \"\"\"Calculates the feature importance by visiting each node in the tree recursively\"\"\"\n",
    "        if node is not None:\n",
    "            self.feature_importances[node.feature_idx] += node.feature_importance\n",
    "            self._calculate_feature_importance(node.left)\n",
    "            self._calculate_feature_importance(node.right)         \n",
    "    ```\n",
    "    Aggregates the importance of each feature based on the information gain from all nodes where the feature was used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"feature-importance\"></a>\n",
    "## 5. Feature Importance\n",
    "\n",
    "Feature importance indicates how valuable each feature is in making predictions. In this implementation, it's calculated based on the information gain contributed by each feature across all splits in the tree.\n",
    "\n",
    "**Accessing Feature Importances:**\n",
    "\n",
    "```python\n",
    "feature_importances = tree.feature_importances\n",
    "for feature, importance in feature_importances.items():\n",
    "    print(f\"Feature {feature}: Importance {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Example with Iris Dataset\n",
    "\n",
    "Let's demonstrate the usage of the `DecisionTree` classifier on the Iris dataset.\n",
    "\n",
    "### 6.1. Importing Necessary Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "\n",
    "# Feature names for reference\n",
    "feature_names = iris.feature_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Splitting the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4. Training the Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Decision Tree\n",
    "tree = DecisionTree(max_depth=5, min_samples_leaf=2, min_information_gain=0.01, numb_of_features_splitting='sqrt')\n",
    "\n",
    "# Train the tree\n",
    "tree.train(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5. Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "predictions = tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6. Evaluating the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        19\n",
      "  versicolor       1.00      1.00      1.00        13\n",
      "   virginica       1.00      1.00      1.00        13\n",
      "\n",
      "    accuracy                           1.00        45\n",
      "   macro avg       1.00      1.00      1.00        45\n",
      "weighted avg       1.00      1.00      1.00        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(Y_test, predictions)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(Y_test, predictions, target_names=iris.target_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.7. Visualizing the Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Structure:\n",
      "                                                                -> LEAF | Label Counts = 0.0->5 | Pred Probs = [1. 0. 0.]\n",
      "                                                -> NODE | Information Gain = 0.3059584928680418 | Split IF X[3] < 0.25 THEN left O/W right\n",
      "                                                                -> LEAF | Label Counts = 0.0->1, 1.0->1 | Pred Probs = [0.5 0.5 0. ]\n",
      "                                -> NODE | Information Gain = 0.07119390399783052 | Split IF X[1] < 3.1 THEN left O/W right\n",
      "                                                -> LEAF | Label Counts = 0.0->25 | Pred Probs = [1. 0. 0.]\n",
      "                -> NODE | Information Gain = 0.7048798707799149 | Split IF X[2] < 3.15 THEN left O/W right\n",
      "                                -> LEAF | Label Counts = 1.0->11 | Pred Probs = [0. 1. 0.]\n",
      "-> NODE | Information Gain = 0.6559276560334977 | Split IF X[3] < 1.3 THEN left O/W right\n",
      "                                                                                -> LEAF | Label Counts = 1.0->3 | Pred Probs = [0. 1. 0.]\n",
      "                                                                -> NODE | Information Gain = 0.3219280948873623 | Split IF X[3] < 1.5 THEN left O/W right\n",
      "                                                                                -> LEAF | Label Counts = 1.0->1, 2.0->1 | Pred Probs = [0.  0.5 0.5]\n",
      "                                                -> NODE | Information Gain = 0.10430778600809093 | Split IF X[0] < 5.6 THEN left O/W right\n",
      "                                                                -> LEAF | Label Counts = 1.0->16 | Pred Probs = [0. 1. 0.]\n",
      "                                -> NODE | Information Gain = 0.1873311857361487 | Split IF X[2] < 4.9 THEN left O/W right\n",
      "                                                                -> LEAF | Label Counts = 1.0->2 | Pred Probs = [0. 1. 0.]\n",
      "                                                -> NODE | Information Gain = 0.31127812445913283 | Split IF X[2] < 4.975 THEN left O/W right\n",
      "                                                                                -> LEAF | Label Counts = 1.0->2, 2.0->2 | Pred Probs = [0.  0.5 0.5]\n",
      "                                                                -> NODE | Information Gain = 0.2516291673878229 | Split IF X[2] < 5.475 THEN left O/W right\n",
      "                                                                                -> LEAF | Label Counts = 2.0->2 | Pred Probs = [0. 0. 1.]\n",
      "                -> NODE | Information Gain = 0.558327059121984 | Split IF X[3] < 1.8 THEN left O/W right\n",
      "                                                                -> LEAF | Label Counts = 1.0->1, 2.0->1 | Pred Probs = [0.  0.5 0.5]\n",
      "                                                -> NODE | Information Gain = 0.3059584928680418 | Split IF X[2] < 4.85 THEN left O/W right\n",
      "                                                                -> LEAF | Label Counts = 2.0->5 | Pred Probs = [0. 0. 1.]\n",
      "                                -> NODE | Information Gain = 0.07040292390159608 | Split IF X[0] < 6.2 THEN left O/W right\n",
      "                                                -> LEAF | Label Counts = 2.0->26 | Pred Probs = [0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Print the tree structure\n",
    "print(\"Decision Tree Structure:\")\n",
    "tree.print_tree()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.8. Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Importances:\n",
      "sepal length (cm): 0.0476\n",
      "sepal width (cm): 0.0385\n",
      "petal length (cm): 0.2567\n",
      "petal width (cm): 0.6573\n"
     ]
    }
   ],
   "source": [
    "# Display feature importances\n",
    "print(\"\\nFeature Importances:\")\n",
    "for idx, importance in tree.feature_importances.items():\n",
    "    print(f\"{feature_names[idx]}: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "In this notebook, we explored a custom implementation of a Decision Tree classifier in Python. We broke down the code to understand each component's role in building, training, and using the tree for predictions. Finally, we demonstrated the classifier's effectiveness using the Iris dataset, achieving high accuracy and identifying key features influencing the decisions.\n",
    "\n",
    "This implementation provides a foundational understanding of how Decision Trees operate and can be extended or optimized further for more complex applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/bijonguha/concepts_revision/blob/main/pca_from_scratch_n_Scikit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bi2tqgGu9qj-"
   },
   "source": [
    "# Implementing PCA from Scratch & Scikit-Learn\n",
    "\n",
    "> 1. Principal Component Analysis in Python and NumPy\n",
    "> 2. Principal Component Analysis in Python using Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tafSZYi59qkB",
    "papermill": {
     "duration": 0.017101,
     "end_time": "2020-09-28T10:41:26.670058",
     "exception": false,
     "start_time": "2020-09-28T10:41:26.652957",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Overview\n",
    "\n",
    "Why are we implementing PCA from scratch if the algorithm is already available in `scikit-learn`? First, coding something from scratch is the best way to understand it. You may know many ML algorithms, but being able to write it down indicates that you have really mastered it. Second, implementing algorithms from scratch is a common task in ML interviews in tech companies, which makes it a useful skill that a job candidate should practice. Last but not least, it's a fun exercise, right? :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7hP_4RcX9qkC",
    "papermill": {
     "duration": 0.016886,
     "end_time": "2020-09-28T10:41:26.750200",
     "exception": false,
     "start_time": "2020-09-28T10:41:26.733314",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. How PCA works\n",
    "\n",
    "Before jumping to implementation, let's quickly refresh our minds. How does PCA work?\n",
    "\n",
    "PCA is a popular unsupervised algorithm used for dimensionality reduction. In a nutshell, PCA helps you to reduce the number of feature in your dataset by combining the features without loosing too much information. More specifically, PCA finds a linear data transformation that projects the data into a new coordinate system with a fewer dimensions. To capture the most variation in the original data, this projection is done by finding the so-called principal components - eigenvectors of the data's covariance matrix - and multiplying the actual data matrix with a subset of the components. This procedure is what we are going to implement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tBHg_Xpp9qkC"
   },
   "source": [
    "# 3. Implementing PCA from Scratch\n",
    "\n",
    "Let's start the implementation! The only library we need to import is `numpy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gy_YTrX59qkC"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CYJbI8Sm9qkD"
   },
   "source": [
    "In line with object-oriented programming practices, we will implement PCA as a class with a set of methods. We will need the following five:\n",
    "\n",
    "1. `__init__()`: initialize the class object.\n",
    "2. `fit()`: center the data and identify principal components.\n",
    "3. `transform()`: transform new data into the identified components.\n",
    "\n",
    "Let's sketch a class object template. Since we implement functions as class methods, we include `self` argument for each method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HQr7PYjg9qkE"
   },
   "outputs": [],
   "source": [
    "class PCA:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        Find principal components\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def transform(self):\n",
    "        \"\"\"\n",
    "        Transform new data\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MqgZAhej9qkE"
   },
   "source": [
    "Now let's go through each method one by one.\n",
    "\n",
    "The `__init__()` method is run once when the initialize the PCA class object.\n",
    "\n",
    "One thing need to do on the initialization step is to store meta-parameters of our algorithm. For PCA, there is only one meta-parameter we will specify: the number of components. We will save it as `self.num_components`.\n",
    "\n",
    "Apart from the meta-parameters, we will create three placeholders that we will use to store important class attributes:\n",
    "- `self.components`: array with the principal component weights\n",
    "- `self.mean`: mean variable values observed in the training data\n",
    "- `self.variance_share`: proportion of variance explained by principal components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1TDf751Q9qkE"
   },
   "outputs": [],
   "source": [
    "def __init__(self, num_components):\n",
    "    self.num_components = num_components\n",
    "    self.components     = None\n",
    "    self.mean           = None\n",
    "    self.variance_share = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qxhvTXp9qkF"
   },
   "source": [
    "Next, let's implement the `fit()` method - the heart of our PCA class. This method will be applied to a provided dataset to identify and memorize principal components.\n",
    "\n",
    "We will do the following steps:\n",
    "1. Center the data by subtracting the mean values for each variable. Normalizing variables is important to make sure that their impact in the data variation is similar and does not depend on the range of that variable. We will also memorize the mean values as `self.mean` as we will need it later for the data transformation.\n",
    "2. Calculate eigenvectors of the covariance matrix. First, we will use `np.cov()` to get the covariance matrix of the data. Next, we will leverage `np.linalg.eig()` to do the eigenvalue decomposition and obtain both eigenvalues and eigenvectors.\n",
    "3. Sort eigenvalues and eigenvectors in the decreasing order. Since we will use a smaller number of components compared to the number of variables in the original data, we would like to focus on components that reflect more data variation. In our case, eigenvectors that correspond to larger eigenvalues capture more variation.\n",
    "4. Store an array with the top `num_components` components as `self.components`.\n",
    "\n",
    "Finally, we will calculate and memorize the data variation explained by the selected components as `self.variance_share`. This can be computed as a cumulative sum of the corresponding eigenvalues divided by the total sum of eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "97XeClRO9qkF"
   },
   "outputs": [],
   "source": [
    "def fit(self, X):\n",
    "    \"\"\"\n",
    "    Find principal components\n",
    "    \"\"\"\n",
    "\n",
    "    # data centering\n",
    "    self.mean = np.mean(X, axis = 0)\n",
    "    X        -= self.mean\n",
    "\n",
    "    # calculate eigenvalues & vectors\n",
    "    cov_matrix      = np.cov(X.T)\n",
    "    values, vectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "    # sort eigenvalues & vectors\n",
    "    sort_idx = np.argsort(values)[::-1]\n",
    "    values   = values[sort_idx]\n",
    "    vectors  = vectors[:, sort_idx]\n",
    "\n",
    "    # store principal components & variance\n",
    "    self.components = vectors[:self.num_components]\n",
    "    self.variance_share = np.sum(values[:self.num_components]) / np.sum(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YvvFjVsq9qkF"
   },
   "source": [
    "The most difficult part is over! Last but not least, we will implement a method to perform the data transformation.\n",
    "\n",
    "This will be run after calling the `fit()` method on the training data, so we only need to implement two steps:\n",
    "1. Center the new data using the same mean values that we used on the fitting stage.\n",
    "2. Multiply the data matrix with the matrix of the selected components. Note that we will need to transpose the components matrix to ensure the right dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YAHajFBa9qkG"
   },
   "outputs": [],
   "source": [
    "def transform(self, X):\n",
    "    \"\"\"\n",
    "    Transform data\n",
    "    \"\"\"\n",
    "\n",
    "    # data centering\n",
    "    X -= self.mean\n",
    "\n",
    "    # decomposition\n",
    "    return np.dot(X, self.components.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJSQHMoi9qkG"
   },
   "source": [
    "Putting everything together, this is how our implementation looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NHrn5nu19qkG"
   },
   "outputs": [],
   "source": [
    "class PCA:\n",
    "\n",
    "    def __init__(self, num_components):\n",
    "        self.num_components = num_components\n",
    "        self.components     = None\n",
    "        self.mean           = None\n",
    "        self.variance_share = None\n",
    "\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Find principal components\n",
    "        \"\"\"\n",
    "\n",
    "        # data centering\n",
    "        self.mean = np.mean(X, axis = 0)\n",
    "        X        -= self.mean\n",
    "\n",
    "        # calculate eigenvalues & vectors\n",
    "        cov_matrix      = np.cov(X.T)\n",
    "        values, vectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "        # sort eigenvalues & vectors\n",
    "        sort_idx = np.argsort(values)[::-1]\n",
    "        values   = values[sort_idx]\n",
    "        vectors  = vectors[:, sort_idx]\n",
    "\n",
    "        # store principal components & variance\n",
    "        self.components = vectors[:self.num_components]\n",
    "        self.variance_share = np.sum(values[:self.num_components]) / np.sum(values)\n",
    "\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform data\n",
    "        \"\"\"\n",
    "\n",
    "        # data centering\n",
    "        X -= self.mean\n",
    "\n",
    "        # decomposition\n",
    "        return np.dot(X, self.components.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gYwRUxb9qkG"
   },
   "source": [
    "## 3.1. Testing the implementation\n",
    "\n",
    "Now that we have our implementation, let's check whether it actually works. We will generate two toy data samples with 10 features using the `np.random` module to draw feature values from a random Normal distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JGvMMqs49qkH",
    "outputId": "e38e3565-9dd6-4e1e-d818-ed4efdd299a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 10) (500, 10)\n"
     ]
    }
   ],
   "source": [
    "X_old = np.random.normal(loc = 0, scale = 1, size = (1000, 10))\n",
    "X_new = np.random.normal(loc = 0, scale = 1, size = (500, 10))\n",
    "\n",
    "print(X_old.shape, X_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EwiLvjbO9qkH"
   },
   "source": [
    "Now, let's instantiate our PCA class, fit it on the old data and transform both datasets!\n",
    "\n",
    "To see if the algorithm works properly, we will generate four new examples as `X_new`, gradually increasing the feature values from 1 to 5. We expect the label predicted by KNN to increase from 0 to 1, since we are getting closer to examples in `X1`. Let's check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GPiCl8Yq9qkI",
    "outputId": "d281d229-201c-4189-965e-d4b4c02062da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance: 0.8325\n"
     ]
    }
   ],
   "source": [
    "# initialize PCA object\n",
    "pca = PCA(num_components = 8)\n",
    "\n",
    "# fit PCA on old data\n",
    "pca.fit(X_old)\n",
    "\n",
    "# check explained variance\n",
    "print(f\"Explained variance: {pca.variance_share:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRjwQ2vU9qkI"
   },
   "source": [
    "Eight components explain more than 83% of the data variation. Not bad! Let's transform the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tMwo0LVL9qkI",
    "outputId": "4292b74b-266b-4595-ea6c-917f81d9bfd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 8) (500, 8)\n"
     ]
    }
   ],
   "source": [
    "# transform datasets\n",
    "X_old = pca.transform(X_old)\n",
    "X_new = pca.transform(X_new)\n",
    "\n",
    "print(X_old.shape, X_new.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qPTxFL0k9qkJ"
   },
   "source": [
    "Yay! Everything works as expected. The new datasets have eight features instead of the original ten features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EuB3OT-X9qkJ",
    "papermill": {
     "duration": 0.08316,
     "end_time": "2020-09-28T13:15:57.370796",
     "exception": false,
     "start_time": "2020-09-28T13:15:57.287636",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3.2. Closing words\n",
    "\n",
    "This is it! I hope this tutorial helps you to refresh your memory on how PCA works and gives you a good idea on how to implement it yourself. You are now well-equipped to do this exercise on your own!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "syboDXKNGzcr"
   },
   "source": [
    "# 4. Implementing PCA using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ZBezP0xBG5YG"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T3FW9Sn3G_9Z",
    "outputId": "df759b33-996f-4faa-f434-ed2480b62363"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 10)\n"
     ]
    }
   ],
   "source": [
    "X_og = np.random.normal(loc = 0, scale = 1, size = (1000, 10))\n",
    "\n",
    "print(X_og.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hokFMbD3HIhD",
    "outputId": "d240be1c-8294-4ac0-b422-04d8a7f18fd0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.27448657, -0.17495394,  0.74533585, ..., -0.39136819,\n",
       "         1.01472267,  1.43939595],\n",
       "       [-0.56264265, -0.55601839, -1.57928476, ..., -0.35233655,\n",
       "        -2.21608568,  0.74914187],\n",
       "       [-0.21489805,  1.57346515, -0.9773304 , ..., -0.95452321,\n",
       "        -1.45551799,  2.62013341],\n",
       "       ...,\n",
       "       [ 0.68301762, -0.87196634,  1.03661053, ..., -1.46980083,\n",
       "        -0.90053479, -0.27891527],\n",
       "       [ 0.428641  ,  0.6447778 ,  1.02459216, ...,  1.65541009,\n",
       "         1.00745767, -0.33712334],\n",
       "       [-2.25948914, -2.10169474, -0.43741168, ..., -0.92958278,\n",
       "        -0.44492095,  1.05603801]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_og"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "uiuPGpBYHOTw"
   },
   "outputs": [],
   "source": [
    "# Scale data before applying PCA\n",
    "scaling = StandardScaler()\n",
    "\n",
    "# Use fit and transform method\n",
    "scaling.fit(X_og)\n",
    "Scaled_data = scaling.transform(X_og)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fORkZ01BHkp3",
    "outputId": "dca46bea-7f7e-4967-9b54-9acd42317f9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 8)\n"
     ]
    }
   ],
   "source": [
    "# Set the n_components=8\n",
    "principal=PCA(n_components=8)\n",
    "principal.fit(Scaled_data)\n",
    "X_pca = principal.transform(Scaled_data)\n",
    "\n",
    "# Check the dimensions of data after PCA\n",
    "print(X_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XsvkiEhpHqcX",
    "outputId": "a650edc2-ed1e-4bae-946a-3a8b78c4d6c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.11413544 0.10935236 0.10698891 0.10366079 0.10183798 0.09861728\n",
      " 0.09654382 0.09286429]\n"
     ]
    }
   ],
   "source": [
    "print(principal.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LETKjLG-H7Lb",
    "outputId": "9e64115d-baf5-44e4-c544-c9ba59c1852b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8240008651189445"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(principal.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GgzUd6mGIDJ0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "papermill": {
   "duration": 9276.743122,
   "end_time": "2020-09-28T13:15:58.077845",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-09-28T10:41:21.334723",
   "version": "2.1.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "5df68ee951abd9e0af39173387ecc93a151d87015b8ef6c5cd08ce73654c6d54"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
